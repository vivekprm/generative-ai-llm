# Generative AI & llm

## Introduction
Generative AI and LLMs specifically are a general purpose technology. That means that similar to other general purpose technologies like deep learning and electricity, is useful not just for a single application, but for a lot of different applications that span many corners of the economy. 

https://machinelearningmastery.com/the-transformer-model/

## Generative AI & LLM Usecases & Model Lifecycle
Generative AI is a subset of traditional machine learning. And the machine learning models that underpin generative AI have learned these abilities by finding statistical patterns in massive datasets of content that was originally generated by humans. 

![image](https://github.com/vivekprm/generative-ai-llm/assets/2403660/73dfa0f3-e421-456d-ac92-60b4109ba30e)

Large language models have been trained on trillions of words over many weeks and months, and with large amounts of compute power. These foundation models, as we call them, with billions of parameters, exhibit emergent properties beyond language alone, and researchers are unlocking their ability to break down complex tasks, reason, and problem solve.

In the above pic we have collection of foundation models, sometimes called base models and their relative size in terms of their parameters. Think of these parameters as models memory. 

And the more parameters a model has, the more memory, and as it turns out, the more sophisticated the tasks it can perform. 

By either using these models as they are or by applying fine tuning techniques to adapt them to your specific use case, you can rapidly build customized solutions without the need to train a new model from scratch.

The way you interact with language models is quite different than other machine learning and programming paradigms. In those cases, you write computer code with formalized syntax to interact with libraries and APIs. In contrast, large language models are able to take natural language or human written instructions and perform tasks much as a human would.

The text that you pass to an LLM is known as a prompt. The space or memory that is available to the prompt is called the context window, and this is typically large enough for a few thousand words, but differs from model to model. 

![image](https://github.com/vivekprm/generative-ai-llm/assets/2403660/150ed63c-980d-4577-8aba-e46feed3ad82)

In this example, you ask the model to determine where Ganymede is located in the solar system. The prompt is passed to the model, the model then predicts the next words, and because your prompt contained a question, this model generates an answer. The output of the model is called a completion, and the act of using the model to generate text is known as inference. The completion is comprised of the text contained in the original prompt, followed by the generated text. You can see that this model did a good job of answering your question. It correctly identifies that Ganymede is a moon of Jupiter and generates a reasonable answer to your question stating that the moon is located within Jupiter's orbit.

## LLM use cases and tasks
You could be forgiven for thinking that LLMs and generative AI are focused on chats tasks. After all, chatbots are highly visible and getting a lot of attention. 

**Next word prediction is the base concept behind a number of different capabilities**, starting with a basic chatbot. However, you can use this conceptually simple technique for a variety of other tasks within text generation. 

For example, you can ask a model to write an essay based on a prompt, to summarize conversations where you provide the dialogue as part of your prompt and the model uses this data along with its understanding of natural language to generate a summary. 

You can use models for a variety of translation tasks from traditional translation between two different languages, such as French and German, or English and Spanish. 

Or to translate natural language to machine code. For example, you could ask a model to write some Python code that will return the mean of every column in a DataFrame and the model will generate code that you can pass to an interpreter. 

You can use LLMs to carry out smaller, focused tasks like information retrieval. In this example, you ask the model to identify all of the people and places identified in a news article. This is known as **named entity recognition, a word classification**. The understanding of knowledge encoded in the model's parameters allows it to correctly carry out this task and return the requested information to you. 

Finally, an area of active development is **augmenting LLMs by connecting them to external data sources or using them to invoke external APIs**. You can use this ability to provide the model with information it doesn't know from its pre-training and to enable your model to power interactions with the real-world. 

Developers have discovered that as the scale of foundation models grows from hundreds of millions of parameters to billions, even hundreds of billions, the subjective understanding of language that a model possesses also increases. This language understanding stored within the parameters of the model is what processes, reasons, and ultimately solves the tasks you give it, but it's also true that smaller models can be fine tuned to perform well on specific focused tasks. 

The rapid increase in capability that LLMs have exhibited in the past few years is largely due to the architecture that powers them.

It's important to note that generative algorithms are not new. Previous generations of language models made use of an architecture called **recurrent neural networks or RNNs**. RNNs while powerful for their time, were limited by the amount of compute and memory needed to perform well at generative tasks.

Let's look at an example of an RNN carrying out a simple next-word prediction generative task. 

![image](https://github.com/vivekprm/generative-ai-llm/assets/2403660/94152d79-a294-4caf-b787-414af52c3bdb)

With just one previous words seen by the model, the prediction can't be very good. 

![image](https://github.com/vivekprm/generative-ai-llm/assets/2403660/998a3254-2642-43ec-9358-9f6876d1b797)

As you scale the RNN implementation to be able to see more of the preceding words in the text, you have to significantly scale the resources that the model uses. As for the prediction, well, the model failed here. 

![image](https://github.com/vivekprm/generative-ai-llm/assets/2403660/d3f4d7bd-c5f8-4ca7-935f-0ef8322b411d)

Even though you scale the model, it still hasn't seen enough of the input to make a good prediction. To successfully predict the next word, models need to see more than just the previous few words. Models needs to have an understanding of the whole sentence or even the whole document. The problem here is that language is complex. In many languages, one word can have multiple meanings. These are homonyms. 

![image](https://github.com/vivekprm/generative-ai-llm/assets/2403660/c0e05e4a-8077-4eb1-a8a3-08f48933d22c)

In this case, it's only with the context of the sentence that we can see what kind of bank is meant. Words within a sentence structures can be ambiguous or have what we might call **syntactic ambiguity**.

![image](https://github.com/vivekprm/generative-ai-llm/assets/2403660/ddb416a6-d444-4c58-b50d-11ce04d3a9fe)

Take for example this sentence, "The teacher taught the students with the book." Did the teacher teach using the book or did the student have the book, or was it both? How can an algorithm make sense of human language if sometimes we can't? 

Well in 2017, after the publication of this paper, Attention is All You Need, from Google and the University of Toronto, everything changed. The transformer architecture had arrived. This novel approach unlocked the progress in generative AI that we see today. 

- It can be scaled efficiently to use multi-core GPUs.
- It can parallel process input data, making use of much larger training datasets.
- And crucially, it's able to learn to pay attention to the meaning of the words it's processing. And attention is all you need. It's in the title.

# Transformers Architecture
Building large language models using the transformer architecture dramatically improved the performance of natural language tasks over the earlier generation of RNNs, and led to an explosion in regenerative capability. 

The power of the transformer architecture lies in its ability to learn the relevance and context of all of the words in a sentence.

![image](https://github.com/vivekprm/generative-ai-llm/assets/2403660/a7655a10-665a-4c4a-825f-4a4c64931266)

Not just as you see here, to each word next to its neighbor, but to every other word in a sentence.

![image](https://github.com/vivekprm/generative-ai-llm/assets/2403660/0b9e35dd-83a8-4476-a3df-eeabbca7e920)

To apply attention weights to those relationships so that the model learns the relevance of each word to each other words no matter where they are in the input. This gives the algorithm the ability to learn who has the book, who could have the book, and if it's even relevant to the wider context of the document. These attention weights are learned during LLM training.

## Self Attention
This diagram is called an attention map and can be useful to illustrate the attention weights between each word and every other word. Here in this stylized example, you can see that the word book is strongly connected with or paying attention to the word teacher and the word student. 

![image](https://github.com/vivekprm/generative-ai-llm/assets/2403660/09bedf17-52da-48e7-a8f5-6f5125d1f95a)

This is called **self-attention** and the ability to learn attention in this way across the whole input significantly approves the model's ability to encode language. Now that you've seen one of the key attributes of the transformer architecture, self-attention, let's cover at a high level how the model works. 

![image](https://github.com/vivekprm/generative-ai-llm/assets/2403660/db4700fa-36b5-40f0-851c-d535adda6d95)

Below is the simplified diagram of the Transformer Architecture 

![image](https://github.com/vivekprm/generative-ai-llm/assets/2403660/768d4956-c9f9-45f4-8f38-49f690c4e27c)

The transformer architecture is split into two distinct parts, the encoder and the decoder. These components work in conjunction with each other and they share a number of similarities.

Also, note here, the diagram you see above is derived from the original attention is all you need paper. Notice how the inputs to the model are at the bottom and the outputs are at the top, where possible we'll try to remain faithful to this throughout the course.

Now, machine-learning models are just big statistical calculators and they work with numbers, not words. So before passing texts into the model to process, you must first tokenize the words. Simply put, this converts the words into numbers, with each number representing a position in a dictionary of all the possible words that the model can work with. 

![image](https://github.com/vivekprm/generative-ai-llm/assets/2403660/89ee5748-706a-46fb-b748-d89bbc2962c6)

You can choose from multiple tokenization methods. For example, token IDs matching two complete words, or using token IDs to represent parts of words. As you can see here. What's important is that once you've selected a tokenizer to train the model, you must use the same tokenizer when you generate text. 

Now that your input is represented as numbers, you can pass it to the embedding layer. This layer is a trainable vector embedding space, a high-dimensional space where **each token is represented as a vector and occupies a unique location within that space**. Each token ID in the vocabulary is matched to a multi-dimensional vector, and the intuition is that these vectors learn to encode the meaning and context of individual tokens in the input sequence. 

Embedding vector spaces have been used in natural language processing for some time, previous generation language algorithms like Word2vec use this concept. 

Looking back at the sample sequence, you can see that in this simple case, each word has been matched to a token ID, and each token is mapped into a vector.

![image](https://github.com/vivekprm/generative-ai-llm/assets/2403660/112d3649-cd8f-456b-8568-b5f791ba41d2)

In the original transformer paper, the vector size was actually 512, so much bigger than we can fit onto this image. For simplicity, if you imagine a vector size of just three, you could plot the words into a three-dimensional space and see the relationships between those words. 

![image](https://github.com/vivekprm/generative-ai-llm/assets/2403660/901f3ec0-fad2-42d4-8c6d-e3f9391344b6)

You can see now how you can relate words that are located close to each other in the embedding space, and how you can calculate the distance between the words as an angle, which gives the model the ability to mathematically understand language. As you add the token vectors into the base of the encoder or the decoder, you also add positional encoding.

![image](https://github.com/vivekprm/generative-ai-llm/assets/2403660/6f4d09ee-fd24-4dbe-83e7-b9a392415094)

The model processes each of the input tokens in parallel. So by adding the positional encoding, you preserve the information about the word order and don't lose the relevance of the position of the word in the sentence. Once you've summed the input tokens and the positional encodings, you pass the resulting vectors to the self-attention layer.

![image](https://github.com/vivekprm/generative-ai-llm/assets/2403660/17c366db-7e41-4802-bf1b-a7f7a8168c32)

Here, the model analyzes the relationships between the tokens in your input sequence. As you saw earlier, this allows the model to attend to different parts of the input sequence to better capture the contextual dependencies between the words.

The self-attention weights that are learned during training and stored in these layers reflect the importance of each word in that input sequence to all other words in the sequence. But this does not happen just once, the transformer architecture actually has multi-headed self-attention. This means that multiple sets of self-attention weights or heads are learned in parallel independently of each other. The number of attention heads included in the attention layer varies from model to model, but numbers in the range of 12-100 are common. 

The intuition here is that each self-attention head will learn a different aspect of language. For example, one head may see the relationship between the people entities in our sentence. Whilst another head may focus on the activity of the sentence. Whilst yet another head may focus on some other properties such as if the words rhyme. It's important to note that you don't dictate ahead of time what aspects of language the attention heads will learn. 

![image](https://github.com/vivekprm/generative-ai-llm/assets/2403660/e622b96c-6d3c-4d29-b58d-e74817b95829)

Now that all of the attention weights have been applied to your input data, the output is processed through a fully-connected feed-forward network. 

![image](https://github.com/vivekprm/generative-ai-llm/assets/2403660/5eb23571-46c2-462f-87c9-11286304068a)


The output of this layer is a vector of logits proportional to the probability score for each and every token in the tokenizer dictionary. You can then pass these logits to a final softmax layer, where they are normalized into a probability score for each word. 

![image](https://github.com/vivekprm/generative-ai-llm/assets/2403660/1d44d1c0-01c5-4acd-a988-cfdeb1bafbe1)

This output includes a probability for every single word in the vocabulary, so there's likely to be thousands of scores here. One single token will have a score higher than the rest. This is the most likely predicted token. 

# Generating text with transformers
Let's walk through a simple example. In this example, you'll look at a translation task or a sequence-to-sequence task, which incidentally was the original objective of the transformer architecture designers.

![image](https://github.com/vivekprm/generative-ai-llm/assets/2403660/ac26a685-7ddf-4460-8a15-7b84d5e17cd8)

You'll use a transformer model to translate the French phrase [FOREIGN] into English. First, you'll tokenize the input words using this same tokenizer that was used to train the network. These tokens are then added into the input on the encoder side of the network, passed through the embedding layer, and then fed into the multi-headed attention layers. The outputs of the multi-headed attention layers are fed through a feed-forward network to the output of the encoder. At this point, the data that leaves the encoder is a deep representation of the structure and meaning of the input sequence. This representation is inserted into the middle of the decoder to influence the decoder's self-attention mechanisms. Next, a start of sequence token is added to the input of the decoder. This triggers the decoder to predict the next token, which it does based on the contextual understanding that it's being provided from the encoder. The output of the decoder's self-attention layers gets passed through the decoder feed-forward network and through a final softmax output layer. At this point, we have our first token. You'll continue this loop, passing the output token back to the input to trigger the generation of the next token, until the model predicts an end-of-sequence token. At this point, the final sequence of tokens can be detokenized into words, and you have your output. In this case, I love machine learning. 


There are multiple ways in which you can use the output from the softmax layer to predict the next token. These can influence how creative you are generated text is.

While the translation example you explored here used both the encoder and decoder parts of the transformer, you can split these components apart for variations of the architecture. **Encoder-only models** also work as sequence-to-sequence models, but without further modification, the input sequence and the output sequence or the same length. Their use is less common these days, but by adding additional layers to the architecture, you can train encoder-only models to perform classification tasks such as sentiment analysis, **BERT is an example of an encoder-only model**. 

Encoder-decoder models, as you've seen, perform well on sequence-to-sequence tasks such as translation, where the input sequence and the output sequence can be different lengths. You can also scale and train this type of model to perform general text generation tasks. **Examples of encoder-decoder models include BART as opposed to BERT and T5**.

Finally, decoder-only models are some of the most commonly used today. Again, as they have scaled, their capabilities have grown. These models can now generalize to most tasks. Popular decoder-only models include the GPT family of models, BLOOM, Jurassic, LLaMA, and many more. 

You can read the Transformers paper [here](https://arxiv.org/abs/1706.03762).

# Prompting & Prompt Engineering
Just to remind you of some of the terminology.

![image](https://github.com/vivekprm/generative-ai-llm/assets/2403660/a6131aac-20be-4d48-ab72-0fd36200a9fa)

The text that you feed into the model is called the prompt, the act of generating text is known as inference, and the output text is known as the completion. The full amount of text or the memory that is available to use for the prompt is called the context window.

Although the example here shows the model performing well, you'll frequently encounter situations where the model doesn't produce the outcome that you want on the first try. You may have to revise the language in your prompt or the way that it's written several times to get the model to behave in the way that you want. This work to develop and improve the prompt is known as prompt engineering. This is a big topic. 

But one powerful strategy to get the model to produce better outcomes is to include examples of the task that you want the model to carry out inside the prompt. Providing examples inside the context window is called **in-context learning**.

## In-Context Learning (ICL) - zero shot inference
With in-context learning, you can help LLMs learn more about the task being asked by including examples or additional data in the prompt.

Here is a concrete example. Within the prompt shown here, you ask the model to classify the sentiment of a review.

![image](https://github.com/vivekprm/generative-ai-llm/assets/2403660/94532627-5f42-433b-bb5f-788f9f822850)

So whether the review of this movie is positive or negative, the prompt consists of the instruction, "Classify this review," followed by some context, which in this case is the review text itself, and an instruction to produce the sentiment at the end. This method, including your input data within the prompt, is called **zero-shot inference**. The largest of the LLMs are surprisingly good at this, grasping the task to be completed and returning a good answer. In this example, the model correctly identifies the sentiment as positive. Smaller models, on the other hand, can struggle with this.

Here's an example of a completion generated by GPT-2, an earlier smaller version of the model that powers ChatGPT. As you can see, the model doesn't follow the instruction. While it does generate text with some relation to the prompt, the model can't figure out the details of the task and does not identify the sentiment. This is where providing an example within the prompt can improve performance. 

![image](https://github.com/vivekprm/generative-ai-llm/assets/2403660/55bdc08f-0408-4028-bcff-77cec011004d)

This is where providing an example within the prompt can improve performance. Here you can see that the prompt text is longer and now starts with a completed example that demonstrates the tasks to be carried out to the model. After specifying that the model should classify the review, the prompt text includes a sample review. I loved this movie, followed by a completed sentiment analysis. In this case, the review is positive. Next, the prompt states the instruction again and includes the actual input review that we want the model to analyze. You pass this new longer prompt to the smaller model, which now has a better chance of understanding the task you're specifying and the format of the response that you want. The inclusion of a single example is known as one-shot inference, in contrast to the zero-shot prompt you supplied earlier. 
Sometimes a single example won't be enough for the model to learn what you want it to do. So you can extend the idea of giving a single example to include multiple examples. This is known as few-shot inference.

![image](https://github.com/vivekprm/generative-ai-llm/assets/2403660/58d282b4-5d9f-4848-a80b-e25abe9e7bfc)

So in summary, you can engineer your prompts to encourage the model to learn by examples. While the largest models are good at zero-shot inference with no examples, smaller models can benefit from one-shot or few-shot inference that include examples of the desired behavior. But remember the context window because you have a limit on the amount of in-context learning that you can pass into the model. Generally, if you find that your model isn't performing well when, say, including five or six examples, you should try fine-tuning your model instead. Fine-tuning performs additional training on the model using new data to make it more capable of the task you want it to perform. 

You may have to try out a few models to find the right one for your use case. Once you've found the model that is working for you, there are a few settings that you can experiment with to influence the structure and style of the completions that the model generates.

# Generative configuration
Some of the methods and associated configuration parameters that you can use to influence the way that the model makes the final decision about next-word generation. 

If you've used LLMs in playgrounds such as on the Hugging Face website or an AWS, you might have been presented with controls like these to adjust how the LLM behaves.

![image](https://github.com/vivekprm/generative-ai-llm/assets/2403660/7c9c527d-4a4f-4358-bbac-37a92f5ed80d)

Each model exposes a set of configuration parameters that can influence the model's output during inference. Note that these are different than the training parameters which are learned during training time. Instead, these configuration parameters are invoked at inference time and give you control over things like the **maximum number of tokens in the completion**, and how creative the output is. 

**Max new tokens** is probably the simplest of these parameters, and you can use it to limit the number of tokens that the model will generate. You can think of this as putting a cap on the number of times the model will go through the selection process. 

![image](https://github.com/vivekprm/generative-ai-llm/assets/2403660/9bae1689-9c7b-493b-be1b-6a5660c41959)

Here you can see examples of max new tokens being set to 100, 150, or 200. But note how the length of the completion in the example for 200 is shorter. This is because another stop condition was reached, such as the model predicting and end of sequence token. Remember it's max new tokens, not a hard number of new tokens generated. 

The output from the transformer's softmax layer is a probability distribution across the entire dictionary of words that the model uses. Here you can see a selection of words and their probability score next to them.

![image](https://github.com/vivekprm/generative-ai-llm/assets/2403660/5e9a5a9b-3e84-4766-a791-28cfa5dd29c1)

Although we are only showing four words here, imagine that this is a list that carries on to the complete dictionary. Most large language models by default will operate with so-called greedy decoding. This is the simplest form of next-word prediction, where the model will always choose the word with the highest probability. This method can work very well for short generation but is susceptible to repeated words or repeated sequences of words. If you want to generate text that's more natural, more creative and avoids repeating words, you need to use some other controls. Random sampling is the easiest way to introduce some variability. Instead of selecting the most probable word every time with random sampling, the model chooses an output word at random using the probability distribution to weight the selection. For example, in the illustration, the word banana has a probability score of 0.02. With random sampling, this equates to a 2% chance that this word will be selected. By using this sampling technique, we reduce the likelihood that words will be repeated. However, depending on the setting, there is a possibility that the output may be too creative, producing words that cause the generation to wander off into topics or words that just don't make sense.

Note that in some implementations, you may need to disable greedy and enable random sampling explicitly. For example, the Hugging Face transformers implementation that we use in the lab requires that we set do sample to equal true. 

Let's explore top k and top p sampling techniques to help limit the random sampling and increase the chance that the output will be sensible. 

![image](https://github.com/vivekprm/generative-ai-llm/assets/2403660/1dfa1d3b-8d75-418a-bb86-faa1f7b0b015)

Two Settings, top p and top k are sampling techniques that we can use to help limit the random sampling and increase the chance that the output will be sensible. To limit the options while still allowing some variability, you can specify a top k value which instructs the model to choose from only the k tokens with the highest probability. 

![image](https://github.com/vivekprm/generative-ai-llm/assets/2403660/3a8e94aa-5a34-4ab7-bf93-74b7d3e5b772)

In this example here, k is set to three, so you're restricting the model to choose from these three options. The model then selects from these options using the probability weighting and in this case, it chooses donut as the next word. This method can help the model have some randomness while preventing the selection of highly improbable completion words. This in turn makes your text generation more likely to sound reasonable and to make sense. 

Alternatively, you can use the top p setting to limit the random sampling to the predictions whose combined probabilities do not exceed p. 

![image](https://github.com/vivekprm/generative-ai-llm/assets/2403660/6fa49310-4452-4bd2-b723-baed0359cee1)

For example, if you set p to equal 0.3, the options are cake and donut since their probabilities of 0.2 and 0.1 add up to 0.3. The model then uses the random probability weighting method to choose from these tokens. With top k, you specify the number of tokens to randomly choose from, and with top p, you specify the total probability that you want the model to choose from.

One more parameter that you can use to control the randomness of the model output is known as **temperature**. This parameter influences the shape of the probability distribution that the model calculates for the next token. Broadly speaking, the higher the temperature, the higher the randomness, and the lower the temperature, the lower the randomness. 

The temperature value is a scaling factor that's applied within the final softmax layer of the model that impacts the shape of the probability distribution of the next token. In contrast to the top k and top p parameters, changing the temperature actually alters the predictions that the model will make. If you choose a low value of temperature, say less than one, the resulting probability distribution from the softmax layer is more strongly peaked with the probability being concentrated in a smaller number of words. You can see this here in the blue bars beside the table, which show a probability bar chart turned on its side. Most of the probability here is concentrated on the word cake.

![image](https://github.com/vivekprm/generative-ai-llm/assets/2403660/5d89d77e-85b0-421f-9877-786973116a01)

The model will select from this distribution using random sampling and the resulting text will be less random and will more closely follow the most likely word sequences that the model learned during training. 

![image](https://github.com/vivekprm/generative-ai-llm/assets/2403660/95217434-d5c5-4931-abc9-3e039a146681)

If instead you set the temperature to a higher value, say, greater than one, then the model will calculate a broader flatter probability distribution for the next token. Notice that in contrast to the blue bars, the probability is more evenly spread across the tokens. **This leads the model to generate text with a higher degree of randomness and more variability in the output compared to a cool temperature setting**. 

This can help you generate text that sounds more creative. If you leave the temperature value equal to one, this will leave the softmax function as default and the unaltered probability distribution will be used. You've covered a lot of ground so far. You've examined the types of tasks that LLMs are capable of performing and learned about transformers, the model architecture that powers these amazing tools. You've also explored how to get the best possible performance out of these models using prompt engineering and by experimenting with different inference configuration parameters. In the next video, you'll start building on this foundational knowledge by thinking through the steps required to develop and launch an LLM -powered application.

# Generative AI project lifecycle

![image](https://github.com/vivekprm/generative-ai-llm/assets/2403660/fad8128b-14bf-4796-ad91-f57a6d58edd7)

The most important step in any project is to define the scope as accurately and narrowly as you can. As you've seen in this course so far, LLMs are capable of carrying out many tasks, but their abilities depend strongly on the size and architecture of the model. You should think about what function the LLM will have in your specific application. Do you need the model to be able to carry out many different tasks, including long-form text generation or with a high degree of capability, or is the task much more specific like named entity recognition so that your model only needs to be good at one thing.

Once you're happy, and you've scoped your model requirements enough to begin development. Your first decision will be whether to train your own model from scratch or work with an existing base model. In general, you'll start with an existing model, although there are some cases where you may find it necessary to train a model from scratch.

With your model in hand, the next step is to assess its performance and carry out additional training if needed for your application. As you saw earlier, prompt engineering can sometimes be enough to get your model to perform well, so you'll likely start by trying in-context learning, using examples suited to your task and use case. There are still cases, however, where the model may not perform as well as you need, even with one or a few short inference, and in that case, you can try fine-tuning your model.

As models become more capable, it's becoming increasingly important to ensure that they behave well and in a way that is aligned with human preferences in deployment.

An important aspect of all of these techniques is evaluation. You will explore some metrics and benchmarks that can be used to determine how well your model is performing or how well aligned it is to your preferences.

Note that this adapt and aligned stage of app development can be highly iterative. You may start by trying prompt engineering and evaluating the outputs, then using fine tuning to improve performance and then revisiting and evaluating prompt engineering one more time to get the performance that you need.

Finally, when you've got a model that is meeting your performance needs and is well aligned, you can deploy it into your infrastructure and integrate it with your application. At this stage, an important step is to optimize your model for deployment. This can ensure that you're making the best use of your compute resources and providing the best possible experience for the users of your application. The last but very important step is to consider any additional infrastructure that your application will require to work well. There are some fundamental limitations of LLMs that can be difficult to overcome through training alone like their tendency to invent information when they don't know an answer, or their limited ability to carry out complex reasoning and mathematics. 

https://aws.amazon.com/blogs/aws/generative-ai-with-large-language-models-new-hands-on-course-by-deeplearning-ai-and-aws/
https://huggingface.co

# Lab
In apache sagemaker terminal run:
```sh
aws s3 cp --recursive s3://dlai-generative-ai/labs/w1-549876/ ./
```
